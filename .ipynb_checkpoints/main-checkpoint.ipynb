{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collaborative-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Constants used throughout the file\n",
    "\"\"\"\n",
    "\n",
    "covid_data_url = \"https://raw.githubusercontent.com/datasets/covid-19/main/data/countries-aggregated.csv\"\n",
    "\n",
    "covid_raw_data_path = \"./data/raw_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pointed-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetchData(url, file_path = covid_raw_data_path):\n",
    "    file = open(file_path, 'w')\n",
    "    file.write(requests.get(url).text)\n",
    "    file.close()\n",
    "\n",
    "fetchData(covid_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "employed-imperial",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.context.SparkContext'>\n",
      "91393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Date,Country,Confirmed,Recovered,Deaths',\n",
       " '2020-01-22,Afghanistan,0,0,0',\n",
       " '2020-01-23,Afghanistan,0,0,0',\n",
       " '2020-01-24,Afghanistan,0,0,0',\n",
       " '2020-01-25,Afghanistan,0,0,0',\n",
       " '2020-01-26,Afghanistan,0,0,0',\n",
       " '2020-01-27,Afghanistan,0,0,0',\n",
       " '2020-01-28,Afghanistan,0,0,0',\n",
       " '2020-01-29,Afghanistan,0,0,0',\n",
       " '2020-01-30,Afghanistan,0,0,0']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# appName = \"Testing\"\n",
    "# conf = SparkConf().setAppName(appName).setMaster(\"local\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "# print(type(sc))\n",
    "\n",
    "# distFile = sc.textFile(covid_raw_data_path)\n",
    "# print(distFile.count())\n",
    "# distFile.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "median-roman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark var type:  <class 'pyspark.sql.session.SparkSession'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DE_project_1\").getOrCreate()\n",
    "print('spark var type: ', type(spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "specified-flour",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+---------+------+\n",
      "|      Date|    Country|Confirmed|Recovered|Deaths|\n",
      "+----------+-----------+---------+---------+------+\n",
      "|2020-01-22|Afghanistan|        0|        0|     0|\n",
      "|2020-01-23|Afghanistan|        0|        0|     0|\n",
      "|2020-01-24|Afghanistan|        0|        0|     0|\n",
      "|2020-01-25|Afghanistan|        0|        0|     0|\n",
      "|2020-01-26|Afghanistan|        0|        0|     0|\n",
      "|2020-01-27|Afghanistan|        0|        0|     0|\n",
      "|2020-01-28|Afghanistan|        0|        0|     0|\n",
      "|2020-01-29|Afghanistan|        0|        0|     0|\n",
      "|2020-01-30|Afghanistan|        0|        0|     0|\n",
      "|2020-01-31|Afghanistan|        0|        0|     0|\n",
      "|2020-02-01|Afghanistan|        0|        0|     0|\n",
      "|2020-02-02|Afghanistan|        0|        0|     0|\n",
      "|2020-02-03|Afghanistan|        0|        0|     0|\n",
      "|2020-02-04|Afghanistan|        0|        0|     0|\n",
      "|2020-02-05|Afghanistan|        0|        0|     0|\n",
      "|2020-02-06|Afghanistan|        0|        0|     0|\n",
      "|2020-02-07|Afghanistan|        0|        0|     0|\n",
      "|2020-02-08|Afghanistan|        0|        0|     0|\n",
      "|2020-02-09|Afghanistan|        0|        0|     0|\n",
      "|2020-02-10|Afghanistan|        0|        0|     0|\n",
      "+----------+-----------+---------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_schema = [StructField('Date', DateType(), True), \n",
    "                StructField('Country', StringType(), True),\n",
    "                StructField('Confirmed', IntegerType(), True),\n",
    "                StructField('Recovered', IntegerType(), True),\n",
    "                StructField('Deaths', IntegerType(), True)\n",
    "            ]\n",
    "schema = StructType(fields=data_schema)\n",
    "\n",
    "df = spark.read.csv(covid_raw_data_path, header=True, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-lighter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "herbal-stability",
   "metadata": {},
   "source": [
    "#### Country code has to be used in Grafana WorldMap. So the codes are fetched from the file and added to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "virtual-novelty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Countries code present in data:  249\n",
      "+-------------------+-------------+-------------+\n",
      "|            Country|2_letter_code|3_letter_code|\n",
      "+-------------------+-------------+-------------+\n",
      "|        Afghanistan|           AF|          AFG|\n",
      "|            Albania|           AL|          ALB|\n",
      "|            Algeria|           DZ|          DZA|\n",
      "|     American Samoa|           AS|          ASM|\n",
      "|            Andorra|           AD|          AND|\n",
      "|             Angola|           AO|          AGO|\n",
      "|           Anguilla|           AI|          AIA|\n",
      "|         Antarctica|           AQ|          ATA|\n",
      "|Antigua and Barbuda|           AG|          ATG|\n",
      "|          Argentina|           AR|          ARG|\n",
      "|            Armenia|           AM|          ARM|\n",
      "|              Aruba|           AW|          ABW|\n",
      "|          Australia|           AU|          AUS|\n",
      "|            Austria|           AT|          AUT|\n",
      "|         Azerbaijan|           AZ|          AZE|\n",
      "|            Bahamas|           BS|          BHS|\n",
      "|            Bahrain|           BH|          BHR|\n",
      "|         Bangladesh|           BD|          BGD|\n",
      "|           Barbados|           BB|          BRB|\n",
      "|            Belarus|           BY|          BLR|\n",
      "+-------------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import Row\n",
    "df_country_code = []\n",
    "country = Row('Country', '2_letter_code', '3_letter_code')\n",
    "with open('./country_code.txt', 'r') as codes:\n",
    "    lines = codes.readlines()\n",
    "    for line in lines:\n",
    "        row = line.split('\\t')[:3]\n",
    "        df_country_code.append(country(row[0], row[1], row[2]))\n",
    "        \n",
    "print('No of Countries code present in data: ', len(df_country_code))\n",
    "df_country_code = spark.createDataFrame(df_country_code)\n",
    "df_country_code.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-configuration",
   "metadata": {},
   "source": [
    "#### Adding country code to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "homeless-welcome",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89280"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = df.join(df_country_code, on='Country')\n",
    "dff.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "terminal-force",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.select(\"Country\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "comparable-jonathan",
   "metadata": {},
   "source": [
    "## SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "trained-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.createOrReplaceTempView(\"covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "royal-warehouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   89280|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "            SELECT COUNT(*) FROM covid\n",
    "        \"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "applicable-asset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT Date)|\n",
      "+--------------------+\n",
      "|                 480|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "            SELECT COUNT(DISTINCT Date) FROM covid\n",
    "        \"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "forbidden-equivalent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "| min(Date)| max(Date)|\n",
      "+----------+----------+\n",
      "|2020-01-22|2021-05-15|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "            SELECT MIN(Date), MAX(Date) FROM covid\n",
    "        \"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "colored-construction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Country)|\n",
      "+-----------------------+\n",
      "|                    186|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "            SELECT COUNT(DISTINCT Country) FROM covid\n",
    "        \"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "present-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------+-------------+----------+-------------+-------------+\n",
      "|      Date|Country|Cum_Confirmed|Cum_Recovered|Cum_Deaths|2_letter_code|3_letter_code|\n",
      "+----------+-------+-------------+-------------+----------+-------------+-------------+\n",
      "|2020-01-22|  India|            0|            0|         0|           IN|          IND|\n",
      "|2020-01-23|  India|            0|            0|         0|           IN|          IND|\n",
      "|2020-01-24|  India|            0|            0|         0|           IN|          IND|\n",
      "|2020-01-25|  India|            0|            0|         0|           IN|          IND|\n",
      "|2020-01-26|  India|            0|            0|         0|           IN|          IND|\n",
      "|2020-01-27|  India|            0|            0|         0|           IN|          IND|\n",
      "|2020-01-28|  India|            0|            0|         0|           IN|          IND|\n",
      "|2020-01-29|  India|            0|            0|         0|           IN|          IND|\n",
      "|2020-01-30|  India|            1|            0|         0|           IN|          IND|\n",
      "|2020-01-31|  India|            1|            0|         0|           IN|          IND|\n",
      "|2020-02-01|  India|            1|            0|         0|           IN|          IND|\n",
      "|2020-02-02|  India|            2|            0|         0|           IN|          IND|\n",
      "|2020-02-03|  India|            3|            0|         0|           IN|          IND|\n",
      "|2020-02-04|  India|            3|            0|         0|           IN|          IND|\n",
      "|2020-02-05|  India|            3|            0|         0|           IN|          IND|\n",
      "|2020-02-06|  India|            3|            0|         0|           IN|          IND|\n",
      "|2020-02-07|  India|            3|            0|         0|           IN|          IND|\n",
      "|2020-02-08|  India|            3|            0|         0|           IN|          IND|\n",
      "|2020-02-09|  India|            3|            0|         0|           IN|          IND|\n",
      "|2020-02-10|  India|            3|            0|         0|           IN|          IND|\n",
      "+----------+-------+-------------+-------------+----------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "            SELECT Date, Country, Confirmed as Cum_Confirmed, Recovered as Cum_Recovered, Deaths as Cum_Deaths, 2_letter_code, 3_letter_code\n",
    "            FROM covid\n",
    "            WHERE Country='India'\n",
    "            ORDER BY Country ASC, Date ASC\n",
    "        \"\"\"\n",
    "dff = spark.sql(query)\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-companion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "square-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+---------+------+-------------+-------------+----------+-------------+-------------+\n",
      "|      Date|    Country|Confirmed|Recovered|Deaths|Cur_Confirmed|Cur_Recovered|Cur_Deaths|2_letter_code|3_letter_code|\n",
      "+----------+-----------+---------+---------+------+-------------+-------------+----------+-------------+-------------+\n",
      "|2020-01-23|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-01-24|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-01-25|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-01-26|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-01-27|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-01-28|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-01-29|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-01-30|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-01-31|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-02-01|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-02-02|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-02-03|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-02-04|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-02-05|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-02-06|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-02-07|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-02-08|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-02-09|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-02-10|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "|2020-02-11|Afghanistan|        0|        0|     0|            0|            0|         0|           AF|          AFG|\n",
      "+----------+-----------+---------+---------+------+-------------+-------------+----------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "            SELECT Date,\n",
    "            Country,\n",
    "            Confirmed,\n",
    "            Recovered,\n",
    "            Deaths,\n",
    "            Confirmed - LAG(Confirmed,1) OVER(PARTITION BY Country  ORDER BY Date) AS Cur_Confirmed,\n",
    "            Recovered - LAG(Recovered,1) OVER(PARTITION BY Country  ORDER BY Date) AS Cur_Recovered,\n",
    "            Deaths - LAG(Deaths,1) OVER(PARTITION BY Country  ORDER BY Date) AS Cur_Deaths,\n",
    "            2_letter_code, \n",
    "            3_letter_code\n",
    "            FROM covid\n",
    "            ORDER BY Country ASC, Date ASC\n",
    "        \"\"\"\n",
    "df_country_wise_agg = spark.sql(query)\n",
    "df_country_wise_agg = df_country_wise_agg.dropna()\n",
    "df_country_wise_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "processed-dominant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89094"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country_wise_agg.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-florida",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "multiple-hostel",
   "metadata": {},
   "source": [
    "### Saving data to MySQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "proprietary-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import mysql.connector as mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "classical-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_config = {\n",
    "    'user': \"Ubuntu\",\n",
    "    'password': \"password\",\n",
    "}\n",
    "DB_NAME = 'COVID_19_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "three-thesaurus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------------> SQL <------------------#\n",
      "Database COVID_19_data does not exists.\n",
      "Database COVID_19_data created successfully.\n",
      "Creating table country_wise_data: OK\n",
      "Saving data to SQL... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89094/89094 [00:38<00:00, 2334.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"#------------------> SQL <------------------#\")\n",
    "\n",
    "TABLES = {}\n",
    "TABLES['country_wise_data'] = (\n",
    "    \"CREATE TABLE `country_wise_data` (\"\n",
    "    \"  `Date` DATE,\"\n",
    "    \"  `Country` VARCHAR(100),\"\n",
    "    \"  `Tot_Confirmed` BIGINT,\"\n",
    "    \"  `Tot_Recovered` BIGINT,\"\n",
    "    \"  `Tot_Deaths` BIGINT,\"\n",
    "    \"  `Confirmed` BIGINT,\"\n",
    "    \"  `Recovered` BIGINT,\"\n",
    "    \"  `Deaths` BIGINT,\"\n",
    "    \"  `2_letter_code` VARCHAR(2),\"\n",
    "    \"  `3_letter_code` VARCHAR(3)\"\n",
    "    \") ENGINE=InnoDB\")\n",
    "\n",
    "\n",
    "cnx = mySQL.connect(**DB_config)\n",
    "cursor = cnx.cursor(buffered=True)\n",
    "# cursor.execute('SET GLOBAL max_allowed_packet=67108864')\n",
    "\n",
    "def create_database(cursor):\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            \"CREATE DATABASE {} DEFAULT CHARACTER SET 'utf8'\".format(DB_NAME))\n",
    "    except mysql.connector.Error as err:\n",
    "        print(\"Failed creating database: {}\".format(err))\n",
    "        exit(1)\n",
    "\n",
    "try:\n",
    "    cursor.execute(\"USE {}\".format(DB_NAME))\n",
    "except mySQL.Error as err:\n",
    "    print(\"Database {} does not exists.\".format(DB_NAME))\n",
    "    if err.errno == mySQL.errorcode.ER_BAD_DB_ERROR:\n",
    "        create_database(cursor)\n",
    "        print(\"Database {} created successfully.\".format(DB_NAME))\n",
    "        cnx.database = DB_NAME\n",
    "    else:\n",
    "        print(err)\n",
    "        exit(1)\n",
    "\n",
    "for table_name in TABLES:\n",
    "    table_description = TABLES[table_name]\n",
    "    try:\n",
    "        print(\"Creating table {}: \".format(table_name), end='')\n",
    "        cursor.execute(table_description)\n",
    "    except mySQL.Error as err:\n",
    "        if err.errno == mySQL.errorcode.ER_TABLE_EXISTS_ERROR:\n",
    "            print(\"already exists.\")\n",
    "        else:\n",
    "            print(err.msg)\n",
    "    else:\n",
    "        print(\"OK\")\n",
    "\n",
    "print(\"Saving data to SQL...\", end=' ')\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO country_wise_data(Date, Country, Tot_Confirmed, Tot_Recovered, Tot_Deaths, Confirmed, Recovered, Deaths, 2_letter_code, 3_letter_code) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "\"\"\"\n",
    "for row in tqdm(df_country_wise_agg.rdd.collect()):\n",
    "    cursor.execute(insert_query ,tuple(row))\n",
    "    \n",
    "print('Done')\n",
    "cnx.commit()\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-wagon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
